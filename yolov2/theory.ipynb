{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding YOLO v2: Better, Faster, Stronger\n",
    "\n",
    "YOLO v2 represents a significant improvement over YOLO v1 in object detection. I'll explain the key concepts, improvements, and technical details in depth.\n",
    "\n",
    "## Recap of YOLO v1\n",
    "\n",
    "YOLO (You Only Look Once) v1 was a single-shot detector that treated object detection as a regression problem. Unlike region-based approaches, YOLO v1:\n",
    "\n",
    "- Processed the entire image in a single pass\n",
    "- Used a grid-based approach (dividing the image into 7×7 grid cells)\n",
    "- Each grid cell predicted 2 bounding boxes and confidence scores\n",
    "- Used a unified architecture to predict bounding boxes and class probabilities\n",
    "- Provided class probability distribution for each grid cell (not per box)\n",
    "\n",
    "The main limitation was that YOLO v1 could only detect a maximum of 49 objects (7×7 grid cells) since each cell was responsible for only one object, regardless of how many boxes it predicted.\n",
    "\n",
    "## YOLO v2 Improvements\n",
    "\n",
    "YOLO v2 aimed to address several limitations of YOLO v1:\n",
    "\n",
    "1. **Accuracy**: YOLO v1 was faster than Faster R-CNN but significantly less accurate (~10 mAP difference)\n",
    "2. **Localization issues**: Poor bounding box placement\n",
    "3. **Recall rate**: Missed too many objects\n",
    "\n",
    "Through a series of modifications, YOLO v2 improved mAP from 63.4% to 78.6% while maintaining speed advantages.\n",
    "\n",
    "## Key Modifications in YOLO v2\n",
    "\n",
    "### 1. Batch Normalization\n",
    "\n",
    "- Added batch normalization to all convolutional layers\n",
    "- Stabilized training and improved convergence\n",
    "- Provided regularization effect, eliminating the need for dropout\n",
    "- Result: +2% mAP improvement\n",
    "\n",
    "### 2. High-Resolution Classifier\n",
    "\n",
    "YOLO v1's training process:\n",
    "- Pre-trained on ImageNet at 224×224 resolution\n",
    "- Directly fine-tuned for detection at 448×448 resolution\n",
    "\n",
    "YOLO v2's improved approach:\n",
    "- Pre-trained on ImageNet at 224×224 resolution\n",
    "- Fine-tuned the classifier on ImageNet at 448×448 for 10 epochs\n",
    "- Then fine-tuned for detection at 448×448\n",
    "- Result: +4% mAP improvement\n",
    "\n",
    "This gradual resolution increase helped the network adapt better to higher resolution inputs.\n",
    "\n",
    "### 3. Convolutional With Anchor Boxes\n",
    "\n",
    "YOLO v1 used fully connected layers for the final prediction, limiting the number of boxes it could predict (98 boxes total).\n",
    "\n",
    "YOLO v2 made several changes:\n",
    "- Removed the fully connected layers\n",
    "- Made the network fully convolutional\n",
    "- Increased grid resolution from 7×7 to 13×13\n",
    "- Introduced anchor boxes (predefined box shapes)\n",
    "- Predicted class probabilities per box rather than per cell\n",
    "- Result: Significant improvement in recall rate\n",
    "\n",
    "### 4. Dimension Clusters (Anchor Box Selection)\n",
    "\n",
    "Rather than using manually defined anchor boxes, YOLO v2 used k-means clustering on the training dataset's ground truth boxes to determine optimal anchor box shapes:\n",
    "\n",
    "- Run k-means clustering on all ground truth bounding boxes\n",
    "- Found that 5 anchor boxes offered the best trade-off\n",
    "- The clustered anchors had better IoU with ground truth boxes than 9 hand-picked anchors in Faster R-CNN\n",
    "- Result: More efficient detection with fewer anchors\n",
    "\n",
    "### 5. Direct Location Prediction\n",
    "\n",
    "YOLO v2 modified how it predicted box coordinates:\n",
    "- YOLO v1: Directly predicted coordinates relative to image\n",
    "- Faster R-CNN: Predicted offsets relative to anchor boxes\n",
    "- YOLO v2: Hybrid approach:\n",
    "  - Predicted x,y coordinates relative to grid cell (using sigmoid to constrain values 0-1)\n",
    "  - Predicted width/height as multipliers of anchor box dimensions (using exponential function to ensure positive values)\n",
    "\n",
    "This approach prevented the network from predicting boxes far from the grid cell's position, improving stability and accuracy.\n",
    "\n",
    "### 6. Fine-Grained Features (Pass-through Layer)\n",
    "\n",
    "To improve detection of small objects:\n",
    "- Added a \"pass-through\" layer that brought features from earlier in the network\n",
    "- Connected a 26×26×512 feature map from an earlier layer to the 13×13 feature map\n",
    "- Reshaped the 26×26×512 into 13×13×2048 and concatenated with existing features\n",
    "- Result: +1% mAP improvement\n",
    "\n",
    "### 7. Multi-Scale Training\n",
    "\n",
    "Because YOLO v2 was fully convolutional, it could process different image sizes:\n",
    "- During training, randomly changed input resolution every 10 batches\n",
    "- Used resolutions from 288×288 to 608×608 (multiples of 32)\n",
    "- Result: Model learned to predict well at different scales\n",
    "- Provided a speed/accuracy trade-off at inference time\n",
    "\n",
    "### New Backbone: Darknet-19\n",
    "\n",
    "YOLO v2 introduced a new classification backbone:\n",
    "- 19 convolutional layers (hence the name)\n",
    "- Fewer parameters than VGG-16 (used in many other detectors)\n",
    "- 5.58 billion operations vs 30.69 billion for VGG-16\n",
    "- Achieved 72.9% top-1 accuracy on ImageNet (comparable to GoogleNet)\n",
    "- Faster than the original YOLO backbone\n",
    "\n",
    "## Technical Details of YOLO v2 Architecture\n",
    "\n",
    "### Network Output Format\n",
    "YOLO v2's output for a 13×13 grid with 5 anchor boxes and 20 classes:\n",
    "- 13×13×125 tensor (125 = 5×(5+20))\n",
    "- For each grid cell and anchor box combination:\n",
    "  - 5 values: tx, ty, tw, th, confidence score\n",
    "  - 20 values: class probabilities\n",
    "- Total parameters: 13×13×5×(5+20) = 21,125\n",
    "\n",
    "### Target Calculation\n",
    "For each ground truth object:\n",
    "- Assign it to the grid cell containing its center\n",
    "- Assign it to the anchor box with highest IoU\n",
    "- Set target values:\n",
    "  - tx, ty: object center relative to grid cell (0 to 1)\n",
    "  - tw, th: width/height relative to anchor box dimension\n",
    "  - confidence: 1 for boxes with objects, 0 otherwise\n",
    "  - class probabilities: one-hot encoded vector for the correct class\n",
    "\n",
    "### Loss Function\n",
    "YOLO v2 used a multi-part squared error loss function:\n",
    "1. Loss for grid cells with no objects (objectness score)\n",
    "2. Anchor box alignment loss (only for first 12,000 iterations)\n",
    "3. Bounding box coordinate loss (x,y,w,h)\n",
    "4. Object confidence score loss\n",
    "5. Class probability loss\n",
    "\n",
    "### Performance Comparison\n",
    "On Pascal VOC dataset:\n",
    "- YOLO v2 (608×608): 78.6% mAP at 40 FPS\n",
    "- SSD512: 76.8% mAP at 22 FPS\n",
    "- Faster R-CNN: 73.2% mAP at 7 FPS\n",
    "\n",
    "On COCO dataset:\n",
    "- YOLO v2: 44.0% mAP, slightly behind SSD512 and Faster R-CNN\n",
    "- Still 3-4× faster than competing methods\n",
    "\n",
    "## Key Improvements Summary\n",
    "\n",
    "1. **Batch Normalization**: +2.0% mAP\n",
    "2. **High-resolution classifier**: +4.0% mAP\n",
    "3. **Convolutional with anchor boxes**: +0.3% mAP (but improved recall)\n",
    "4. **Dimension clusters (k-means anchors)**: +5.0% mAP\n",
    "5. **Direct location prediction**: +1.0% mAP\n",
    "6. **Fine-grained features (pass-through)**: +1.0% mAP\n",
    "7. **Multi-scale training**: +1.3% mAP\n",
    "\n",
    "The cumulative effect was a 15.2% mAP improvement over YOLO v1 while maintaining high frame rates.\n",
    "\n",
    "In summary, YOLO v2 represents a significant advancement in real-time object detection, offering a better balance between accuracy and speed compared to its predecessor and competitors. Its innovations in anchor box determination, feature extraction, and training methodology have influenced many subsequent object detection models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
